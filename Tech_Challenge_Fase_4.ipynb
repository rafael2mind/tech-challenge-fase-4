{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8RvUdJG496P"
      },
      "source": [
        "# Tech Challenge - FIAP: IA para Devs (Fase 4)\n",
        "\n",
        "###Grupo 38\n",
        "- Rafael Silva Souza\n",
        "- Rodrigo de Freitas Ornellas\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üîó C√≥digo Github\n",
        "\n",
        "\n",
        "\n",
        "https://github.com/rafael2mind/tech-challenge-fase-4\n",
        "\n",
        "\n",
        "### üîó V√≠deo de apresenta√ß√£o\n",
        "https://youtu.be/3iO-xPl9UFk\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Introdu√ß√£o\n",
        "\n",
        "#### Contexto e Motiva√ß√£o\n",
        "\n",
        "Com o avan√ßo das tecnologias de vis√£o computacional e a populariza√ß√£o de aplica√ß√µes baseadas em intelig√™ncia artificial, torna-se cada vez mais importante explorar novas formas de extrair informa√ß√µes valiosas de v√≠deos. No contexto atual, v√≠deos s√£o amplamente utilizados em diversas √°reas, desde seguran√ßa at√© entretenimento, e a capacidade de analis√°-los de forma automatizada pode trazer insights poderosos e efici√™ncia para diferentes aplica√ß√µes.\n",
        "\n",
        "O Tech Challenge da fase 4 prop√µe o desenvolvimento de uma aplica√ß√£o que utilize t√©cnicas avan√ßadas para an√°lise de v√≠deo, englobando reconhecimento facial, an√°lise de express√µes emocionais, detec√ß√£o de atividades e gera√ß√£o de relat√≥rios automatizados. Essa proposta visa consolidar o aprendizado em disciplinas como vis√£o computacional e machine learning, promovendo a integra√ß√£o pr√°tica de conhecimentos para resolver problemas do mundo real.\n",
        "\n",
        "\n",
        "#### Objetivos do Projeto\n",
        "\n",
        "O objetivo principal deste projeto √© criar uma aplica√ß√£o de an√°lise de v√≠deo que forne√ßa insights detalhados por meio das seguintes funcionalidades:\n",
        "\n",
        "1. \t**Reconhecimento Facial**: Identificar e marcar os rostos presentes nos v√≠deos, criando um mapa de identifica√ß√£o em cada frame.\n",
        "2.\t**An√°lise de Express√µes Emocionais**: Detectar as emo√ß√µes expressadas pelos rostos identificados e classific√°-las, como felicidade, tristeza ou raiva.\n",
        "3.\t**Detec√ß√£o de Atividades**: Categorizar as atividades realizadas nos v√≠deos, como movimentos f√≠sicos ou intera√ß√µes entre indiv√≠duos.\n",
        "4.\t**Gera√ß√£o de Resumo**: Automatizar a cria√ß√£o de relat√≥rios com as principais atividades e emo√ß√µes detectadas, incluindo estat√≠sticas como o n√∫mero total de frames analisados e a quantidade de anomalias observadas.\n",
        "\n",
        "#### Estrutura do Projeto\n",
        "\n",
        "Este documento est√° organizado da seguinte forma:\n",
        "\n",
        "1. **Introdu√ß√£o**: Apresenta o contexto e a motiva√ß√£o do projeto, bem como seus objetivos principais.\n",
        "2. **Descri√ß√£o do Problema**: Detalha o problema a ser resolvido, sua import√¢ncia no contexto da an√°lise de v√≠deo e os crit√©rios de sucesso estabelecidos.\n",
        "3. **Fundamenta√ß√£o Te√≥rica**: Fornece uma vis√£o geral das t√©cnicas de vis√£o computacional aplicadas, incluindo reconhecimento facial, an√°lise de express√µes emocionais e detec√ß√£o de atividades.\n",
        "4. **Metodologia**: Descreve o processo de an√°lise de v√≠deo, incluindo pr√©-processamento, detec√ß√£o facial, an√°lise emocional e categoriza√ß√£o de atividades.\n",
        "5. **Implementa√ß√£o**: Detalha as ferramentas e bibliotecas utilizadas, bem como a estrutura do c√≥digo e as etapas de desenvolvimento.\n",
        "6. **Conclus√£o**: Resume os achados do projeto, apresenta as conclus√µes principais e aponta poss√≠veis extens√µes para trabalhos futuros.\n",
        "\n",
        "Com esta introdu√ß√£o, estabelecemos as bases para o desenvolvimento do projeto, destacando a relev√¢ncia do problema e os objetivos a serem alcan√ßados. A seguir, aprofundaremos a descri√ß√£o do problema, detalhando as tarefas espec√≠ficas e sua import√¢ncia no contexto da an√°lise de v√≠deo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-suK2_wqbop"
      },
      "source": [
        "###2. Descri√ß√£o do Problema\n",
        "\n",
        "####Defini√ß√£o do Problema\n",
        "\n",
        "O avan√ßo das tecnologias de an√°lise de v√≠deo oferece oportunidades significativas para automatizar tarefas como reconhecimento facial, an√°lise emocional e detec√ß√£o de atividades humanas. Essas capacidades s√£o especialmente valiosas em √°reas como seguran√ßa, marketing e entretenimento, onde a necessidade de extrair informa√ß√µes √∫teis de v√≠deos √© crescente.\n",
        "\n",
        "O problema abordado neste projeto √© o desenvolvimento de uma aplica√ß√£o que processa v√≠deos automaticamente e realiza as seguintes tarefas principais:\n",
        "\n",
        "1.\t**Identificar e marcar rostos presentes nos v√≠deos.**\n",
        "\n",
        "2.\t**Detectar e classificar express√µes emocionais associadas a esses rostos.**\n",
        "\n",
        "3.\t**Categorizar gestos e atividades humanas realizadas no v√≠deo.**\n",
        "\n",
        "4.\t**Gerar um relat√≥rio automatizado em PDF, resumindo as atividades e emo√ß√µes detectadas e apresentando m√©tricas detalhadas.**\n",
        "\n",
        "####Import√¢ncia do Problema para a An√°lise de V√≠deo\n",
        "\n",
        "A relev√¢ncia da an√°lise automatizada de v√≠deos √© evidente em diversos cen√°rios pr√°ticos, com impacto significativo em:\n",
        "\n",
        "1.\t**Seguran√ßa e Vigil√¢ncia:** A detec√ß√£o de atividades suspeitas ou an√¥malas pode melhorar a efici√™ncia de sistemas de seguran√ßa, tornando-os mais proativos e menos dependentes de supervis√£o humana.\n",
        "\n",
        "2.\t**Marketing e Publicidade:** A an√°lise de express√µes emocionais ajuda a medir rea√ß√µes do p√∫blico a conte√∫dos promocionais, melhorando a personaliza√ß√£o de campanhas.\n",
        "\n",
        "3.\t**Sa√∫de e Bem-estar:** O reconhecimento de atividades humanas √© √∫til no monitoramento de pacientes e na reabilita√ß√£o, especialmente para detectar padr√µes de movimento at√≠picos.\n",
        "\n",
        "4.\t**Entretenimento e M√≠dia:** Compreender emo√ß√µes e atividades melhora experi√™ncias interativas e fornece insights sobre o comportamento do p√∫blico em tempo real.\n",
        "\n",
        "####Abordagem para Resolu√ß√£o do Problema\n",
        "\n",
        "Para resolver o problema, seguimos estas etapas metodol√≥gicas:\n",
        "\n",
        "1.\t**Coleta e Pr√©-processamento do V√≠deo:**\n",
        "\n",
        "- **Fontes de Dados:** O v√≠deo disponibilizado na plataforma do aluno foi utilizado como fonte principal.\n",
        "\n",
        "- **Prepara√ß√£o dos Dados:** T√©cnicas como ajuste de resolu√ß√£o, convers√£o de cores para RGB e extra√ß√£o de frames foram aplicadas para garantir compatibilidade com os modelos de an√°lise.\n",
        "\n",
        "2.\t**Implementa√ß√£o das Funcionalidades:**\n",
        "\n",
        "- **Reconhecimento Facial:** Utiliza√ß√£o de MediaPipe e face_recognition para detectar e identificar rostos em tempo real.\n",
        "\n",
        "- **An√°lise Emocional:** Uso da biblioteca DeepFace para classificar emo√ß√µes como alegria, tristeza, raiva e surpresa.\n",
        "\n",
        "- **Detec√ß√£o de Atividades e Gestos:** MediaPipe foi empregado para identificar gestos como ‚ÄúWave‚Äù, ‚ÄúHands Up‚Äù e ‚ÄúArms Crossed‚Äù.\n",
        "\n",
        "- **An√°lise de Anomalias:** Identifica√ß√£o de gestos inesperados ou movimentos bruscos para sinalizar comportamentos at√≠picos.\n",
        "\n",
        "3.\t**Gera√ß√£o de Relat√≥rios:**\n",
        "\n",
        "- **Resumo Autom√°tico:** Compila√ß√£o das informa√ß√µes em um relat√≥rio PDF que apresenta m√©tricas, emo√ß√µes e gestos identificados em formato visual e textual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJkK0kUUq3N1"
      },
      "source": [
        "###3. Fundamenta√ß√£o Te√≥rica\n",
        "\n",
        "####An√°lise de V√≠deo Automatizada\n",
        "\n",
        "A an√°lise de v√≠deo automatizada √© uma √°rea em crescimento na vis√£o computacional, que busca extrair informa√ß√µes significativas de v√≠deos de maneira eficiente e precisa. Essa abordagem combina t√©cnicas avan√ßadas, como reconhecimento facial, an√°lise de emo√ß√µes e detec√ß√£o de atividades humanas, para atender a aplica√ß√µes em seguran√ßa, marketing, sa√∫de, entre outros.\n",
        "\n",
        "####Reconhecimento Facial\n",
        "\n",
        "O reconhecimento facial √© uma tecnologia amplamente utilizada para identificar e verificar pessoas com base em caracter√≠sticas faciais √∫nicas. Ele √© composto pelas seguintes etapas:\n",
        "\n",
        "1.\t**Detec√ß√£o Facial:** Identifica√ß√£o de rostos em imagens ou v√≠deos, utilizando ferramentas como:\n",
        "\n",
        "- **MediaPipe Face Detection:** Detecta rostos em tempo real com alta precis√£o e efici√™ncia.\n",
        "\n",
        "- **Face Recognition (dlib):** Extrai caracter√≠sticas faciais para embasamento em reconhecimento.\n",
        "\n",
        "2.\t**Extra√ß√£o de Recursos:** Cria√ß√£o de uma representa√ß√£o √∫nica do rosto, baseada em caracter√≠sticas como dist√¢ncia entre olhos e formato do nariz.\n",
        "\n",
        "3.\t**Classifica√ß√£o e Identifica√ß√£o:** Compara√ß√£o da representa√ß√£o facial extra√≠da com um banco de dados para determinar a identidade.\n",
        "\n",
        "No projeto, a biblioteca MediaPipe √© utilizada para a detec√ß√£o inicial, enquanto a Face Recognition realiza a identifica√ß√£o e associa√ß√£o com rostos conhecidos.\n",
        "\n",
        "####An√°lise de Express√µes Emocionais\n",
        "\n",
        "A an√°lise de express√µes emocionais busca identificar emo√ß√µes humanas, como felicidade, tristeza ou surpresa, com base em padr√µes faciais. As etapas incluem:\n",
        "\n",
        "1.\t**Detec√ß√£o de Regi√µes Faciais:** Identifica√ß√£o de √°reas espec√≠ficas do rosto, como olhos, boca e sobrancelhas.\n",
        "\n",
        "2.\t**Extra√ß√£o de Padr√µes Emocionais:** An√°lise de microexpress√µes e caracter√≠sticas faciais utilizando modelos de aprendizado de m√°quina.\n",
        "\n",
        "3.\t**Classifica√ß√£o de Emo√ß√µes:** Aplica√ß√£o de bibliotecas como DeepFace, que possui modelos pr√©-treinados para detectar emo√ß√µes como raiva, felicidade, neutralidade, entre outras.\n",
        "\n",
        "A biblioteca DeepFace foi empregada no projeto para realizar a an√°lise emocional em tempo real.\n",
        "\n",
        "####Detec√ß√£o de Atividades e Gestos\n",
        "\n",
        "A detec√ß√£o de atividades em v√≠deos busca identificar a√ß√µes humanas, como acenos ou bra√ßos cruzados. No projeto, utilizamos:\n",
        "\n",
        "1.\t**Modelos Baseados em Pose:** A biblioteca MediaPipe Pose foi utilizada para rastrear landmarks (esqueletos humanos) e identificar gestos como:\n",
        "\n",
        "- **Wave (Aceno):** Posi√ß√£o do pulso acima do ombro.\n",
        "\n",
        "- **Hands Up (M√£os levantadas):** Posi√ß√£o do pulso acima do nariz.\n",
        "\n",
        "- **Arms Crossed (Bra√ßos cruzados):** Cruza os bra√ßos diante do tronco.\n",
        "\n",
        "2.\t**Classifica√ß√£o de Atividades:** Baseada em padr√µes geom√©tricos definidos com landmarks corporais.\n",
        "\n",
        "Esses gestos foram mapeados e associados aos rostos detectados, enriquecendo a an√°lise de v√≠deo com contexto de a√ß√µes humanas.\n",
        "\n",
        "####An√°lise de Anomalias em Atividades\n",
        "\n",
        "Anomalias em v√≠deos s√£o padr√µes de movimento que desviam do comportamento esperado, como gestos bruscos ou intera√ß√µes incomuns. A abordagem inclui:\n",
        "\n",
        "1.\t**Modelagem de Comportamentos Normais:** Identifica√ß√£o de padr√µes t√≠picos de movimentos com landmarks rastreados.\n",
        "\n",
        "2.\t**Identifica√ß√£o de Outliers:** Uso de regras baseadas em limiares para detectar desvios significativos, como movimentos inesperados.\n",
        "\n",
        "No contexto do projeto, gestos como acenos ou movimentos espec√≠ficos foram utilizados como base para a an√°lise de comportamentos.\n",
        "\n",
        "####Gera√ß√£o de Relat√≥rios Automatizados\n",
        "\n",
        "A gera√ß√£o de relat√≥rios sintetiza as informa√ß√µes extra√≠das do v√≠deo em um formato estruturado e visualmente compreens√≠vel. Isso inclui:\n",
        "\n",
        "1.\t**Compila√ß√£o de M√©tricas:** Dados como n√∫mero de frames analisados, quantidade de rostos detectados e gestos identificados s√£o organizados em relat√≥rios.\n",
        "\n",
        "2.\t**Automa√ß√£o com PDF:** O relat√≥rio √© gerado automaticamente em formato PDF utilizando a biblioteca fpdf.\n",
        "\n",
        "3.\t**Visualiza√ß√£o dos Resultados:** M√©tricas de desempenho s√£o apresentadas em se√ß√µes claras e formatadas, tornando o relat√≥rio √∫til para diferentes p√∫blicos.\n",
        "\n",
        "####Tecnologias e Ferramentas\n",
        "\n",
        "Para implementar as funcionalidades descritas, as seguintes ferramentas e bibliotecas foram utilizadas:\n",
        "\n",
        "- **OpenCV:** Para pr√©-processamento de frames e manipula√ß√£o de v√≠deos.\n",
        "\n",
        "- **MediaPipe Face Detection e Pose:** Para detec√ß√£o de rostos e rastreamento de landmarks corporais.\n",
        "\n",
        "- **Face Recognition:** Para reconhecimento facial e compara√ß√£o com rostos conhecidos.\n",
        "\n",
        "- **DeepFace:** Para an√°lise de express√µes emocionais.\n",
        "\n",
        "- **FPDF:** Para gera√ß√£o de relat√≥rios estruturados em formato PDF.\n",
        "\n",
        "- **Python:** Linguagem principal para integra√ß√£o e implementa√ß√£o das funcionalidades.\n",
        "\n",
        "####Conclus√£o da Fundamenta√ß√£o Te√≥rica\n",
        "\n",
        "A an√°lise de v√≠deo automatizada combina tecnologias avan√ßadas de vis√£o computacional e aprendizado de m√°quina para resolver problemas complexos. Este projeto integra reconhecimento facial, an√°lise emocional e detec√ß√£o de gestos com a gera√ß√£o de relat√≥rios automatizados, oferecendo uma solu√ß√£o completa com aplica√ß√µes pr√°ticas em seguran√ßa, marketing, sa√∫de e entretenimento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkbpV7v-rDrm"
      },
      "source": [
        "###4. Metodologia\n",
        "\n",
        "####Descri√ß√£o do Conjunto de Dados\n",
        "\n",
        "Para a implementa√ß√£o da aplica√ß√£o de an√°lise de v√≠deo, utilizamos um v√≠deo disponibilizado na plataforma do aluno, projetado para abordar tarefas de reconhecimento facial, an√°lise de express√µes emocionais e detec√ß√£o de atividades. Cada frame do v√≠deo representa uma oportunidade de extrair informa√ß√µes valiosas para compor o relat√≥rio final.\n",
        "\n",
        "As principais caracter√≠sticas do v√≠deo s√£o: intera√ß√µes humanas com varia√ß√µes de express√µes faciais, gestos como acenos e atividades distintas.\n",
        "\n",
        "####Pr√©-processamento dos Dados\n",
        "\n",
        "O pr√©-processamento do v√≠deo foi realizado diretamente durante a an√°lise em tempo real e incluiu as seguintes etapas:\n",
        "\n",
        "1.\t**Leitura do V√≠deo:**\n",
        "\n",
        "- Utilizamos a biblioteca OpenCV para capturar os frames do v√≠deo.\n",
        "\n",
        "2.\t**Convers√£o para RGB:**\n",
        "\n",
        "- Cada frame foi convertido do formato BGR para RGB para ser compat√≠vel com as bibliotecas de an√°lise facial e emocional.\n",
        "\n",
        "3.\t**Ajuste Din√¢mico:**\n",
        "\n",
        "- Resolu√ß√£o original do v√≠deo foi mantida para garantir a precis√£o das detec√ß√µes.\n",
        "\n",
        "4.\t**Detec√ß√£o de Regi√µes de Interesse (ROIs):**\n",
        "\n",
        "- Apenas as √°reas espec√≠ficas dos frames, como rostos e landmarks corporais, foram analisadas para otimizar o desempenho.\n",
        "\n",
        "####Implementa√ß√£o das Funcionalidades\n",
        "\n",
        "####Reconhecimento Facial\n",
        "\n",
        "1.\t**Modelos Utilizados:**\n",
        "\n",
        "- MediaPipe Face Detection para detec√ß√£o inicial dos rostos.\n",
        "\n",
        "- Face Recognition para codifica√ß√£o e identifica√ß√£o facial.\n",
        "\n",
        "2.\t**Processo de Detec√ß√£o:**\n",
        "\n",
        "- Cada frame foi analisado para localizar e marcar os rostos presentes.\n",
        "\n",
        "- Codifica√ß√µes faciais foram comparadas com uma lista de rostos conhecidos para identifica√ß√£o.\n",
        "\n",
        "3.\t**Armazenamento dos Resultados:**\n",
        "\n",
        "- Coordenadas dos rostos e identifica√ß√µes foram registradas no relat√≥rio.\n",
        "\n",
        "####An√°lise de Express√µes Emocionais\n",
        "\n",
        "1.\t**Modelo Utilizado:**\n",
        "\n",
        "- DeepFace, que utiliza modelos pr√©-treinados para classifica√ß√£o de emo√ß√µes como felicidade, tristeza, surpresa, entre outras.\n",
        "\n",
        "2.\t**Processo de Classifica√ß√£o:**\n",
        "\n",
        "- Cada rosto detectado foi analisado para identificar a emo√ß√£o dominante.\n",
        "\n",
        "3.\t**Armazenamento dos Resultados:**\n",
        "\n",
        "- As emo√ß√µes foram registradas junto com os nomes das pessoas detectadas e armazenadas no relat√≥rio.\n",
        "\n",
        "####Detec√ß√£o de Atividades e Gestos\n",
        "\n",
        "1.\t**Modelo Utilizado:**\n",
        "\n",
        "- MediaPipe Pose foi utilizado para rastrear landmarks corporais e categorizar gestos.\n",
        "\n",
        "2.\t**Processo de Detec√ß√£o:**\n",
        "\n",
        "- Landmarks dos membros superiores e superiores foram analisados para identificar gestos, como:\n",
        "\n",
        "- **Wave (Aceno):** Pulso acima do ombro.\n",
        "\n",
        "- **Hands Up (M√£os levantadas):** Pulso acima do nariz.\n",
        "\n",
        "- **Arms Crossed (Bra√ßos cruzados):** Cruza os bra√ßos diante do tronco.\n",
        "\n",
        "- **Pointing (Apontando):** Dedos direcionados para frente.\n",
        "\n",
        "3.\t**Classifica√ß√£o:**\n",
        "\n",
        "- Gestos detectados foram associados aos rostos mais pr√≥ximos e registrados no relat√≥rio.\n",
        "\n",
        "####Gera√ß√£o de Relat√≥rios\n",
        "\n",
        "1.\t**Compila√ß√£o de M√©tricas:**\n",
        "\n",
        "- M√©tricas como n√∫mero total de frames analisados, quantidade de rostos detectados, distribui√ß√£o de emo√ß√µes e gestos identificados foram organizadas.\n",
        "\n",
        "2.\t**Automa√ß√£o:**\n",
        "\n",
        "- Relat√≥rios foram gerados automaticamente em formato PDF utilizando a biblioteca FPDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxSHWWJ-sdZY"
      },
      "source": [
        "###5. Implementa√ß√£o\n",
        "\n",
        "**Ferramentas e Tecnologias Utilizadas**\n",
        "\n",
        "Para o desenvolvimento do projeto, empregamos as seguintes ferramentas e tecnologias:\n",
        "\n",
        "1.\t**Linguagem de Programa√ß√£o:**\n",
        "\n",
        "- **Python:** Escolhida por sua vasta gama de bibliotecas voltadas para vis√£o computacional, aprendizado de m√°quina e manipula√ß√£o de dados.\n",
        "\n",
        "2.\t**Bibliotecas de Vis√£o Computacional:**\n",
        "\n",
        "- **OpenCV:** Para pr√©-processamento de v√≠deo, convers√£o de frames e manipula√ß√£o de imagens.\n",
        "\n",
        "- **MediaPipe:** Para detec√ß√£o de rostos e rastreamento de landmarks corporais, como m√£os e gestos.\n",
        "\n",
        "- **Face Recognition:** Para codifica√ß√£o e reconhecimento facial.\n",
        "\n",
        "- **DeepFace:** Para an√°lise de express√µes emocionais.\n",
        "\n",
        "3.\t**Modelos e M√©todos de Detec√ß√£o:**\n",
        "\n",
        "- **MediaPipe Pose:** Para rastreamento de atividades humanas, como gestos e posi√ß√µes corporais.\n",
        "\n",
        "- **DeepFace:** Modelos pr√©-treinados para reconhecimento de emo√ß√µes.\n",
        "\n",
        "4.\t**Visualiza√ß√£o e Relat√≥rios:**\n",
        "\n",
        "- **FPDF:** Para cria√ß√£o e exporta√ß√£o de relat√≥rios em formato PDF.\n",
        "\n",
        "5.\t**Ambiente de Desenvolvimento:**\n",
        "\n",
        "- **Google Colab:** Ambiente baseado em nuvem, que facilita o desenvolvimento, execu√ß√£o e compartilhamento de c√≥digo.\n",
        "\n",
        "- **GitHub:** Para controle de vers√£o e compartilhamento do c√≥digo-fonte.\n",
        "\n",
        "**Estrutura do C√≥digo**\n",
        "\n",
        "O c√≥digo foi organizado em m√≥dulos para facilitar a clareza, reusabilidade e manuten√ß√£o. A estrutura principal √© composta por:\n",
        "\n",
        "1.\t**Pr√©-processamento de V√≠deo:**\n",
        "\n",
        "- Leitura e extra√ß√£o de frames em tempo real utilizando OpenCV.\n",
        "\n",
        "- Convers√£o de frames para RGB para compatibilidade com as bibliotecas Face Recognition e DeepFace.\n",
        "\n",
        "2.\t**Reconhecimento Facial:**\n",
        "\n",
        "- Detec√ß√£o inicial de rostos utilizando MediaPipe.\n",
        "\n",
        "- Codifica√ß√£o e identifica√ß√£o de rostos com Face Recognition.\n",
        "\n",
        "- Registro de coordenadas dos rostos e nomes associados no relat√≥rio.\n",
        "\n",
        "3.\t**An√°lise de Express√µes Emocionais:**\n",
        "\n",
        "- Aplica√ß√£o da biblioteca DeepFace para an√°lise de emo√ß√µes em cada rosto detectado.\n",
        "\n",
        "- Registro de emo√ß√µes como felicidade, tristeza, surpresa, entre outras.\n",
        "\n",
        "4.\t**Detec√ß√£o de Atividades e Gestos:**\n",
        "\n",
        "- Rastreamento de landmarks corporais com MediaPipe Pose.\n",
        "\n",
        "- Identifica√ß√£o de gestos espec√≠ficos, como:\n",
        "\n",
        "- **Wave (Aceno):** Posi√ß√£o do pulso acima do ombro.\n",
        "\n",
        "- **Hands Up (M√£os levantadas):** Posi√ß√£o do pulso acima do nariz.\n",
        "\n",
        "- **Arms Crossed (Bra√ßos cruzados):** Cruza os bra√ßos na frente do tronco.\n",
        "\n",
        "- **Pointing (Apontando):** Dedos apontados em dire√ß√£o a um alvo.\n",
        "\n",
        "- Registro dos gestos no relat√≥rio, associados aos rostos detectados.\n",
        "\n",
        "5.\t**Gera√ß√£o de Relat√≥rio:**\n",
        "\n",
        "- Compila√ß√£o de m√©tricas como:\n",
        "\n",
        "- Total de frames analisados.\n",
        "\n",
        "- N√∫mero de rostos detectados.\n",
        "\n",
        "- Distribui√ß√£o de emo√ß√µes.\n",
        "\n",
        "- Gestos identificados.\n",
        "\n",
        "- Cria√ß√£o de relat√≥rios automatizados em formato PDF utilizando FPDF.\n",
        "\n",
        "**Fluxo de Trabalho do C√≥digo**\n",
        "\n",
        "1.\t**Inicializa√ß√£o do V√≠deo:**\n",
        "\n",
        "- Leitura do v√≠deo de entrada e configura√ß√£o do v√≠deo de sa√≠da.\n",
        "\n",
        "2.\t**Processamento de Frames:**\n",
        "\n",
        "- Cada frame √© analisado para:\n",
        "\n",
        "- Detec√ß√£o de rostos e emo√ß√µes.\n",
        "\n",
        "- Rastreamento de gestos e atividades.\n",
        "\n",
        "3.\t**Registro e Visualiza√ß√£o:**\n",
        "\n",
        "- Informa√ß√µes detectadas s√£o sobrepostas ao v√≠deo em tempo real para visualiza√ß√£o.\n",
        "\n",
        "- Dados s√£o acumulados para inclus√£o no relat√≥rio final.\n",
        "\n",
        "4.\t**Gera√ß√£o de Relat√≥rio:**\n",
        "\n",
        "- Ap√≥s o processamento completo do v√≠deo, os dados coletados s√£o organizados e exportados em PDF."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Etapas da Implementa√ß√£o\n",
        "\n",
        "1. Pr√©-processamento do V√≠deo"
      ],
      "metadata": {
        "id": "VZY2Anify3qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instala√ß√£o de bibliotecas essenciais\n",
        "!pip install -q opencv-python mediapipe tqdm\n",
        "\n",
        "# Instala√ß√£o de bibliotecas para an√°lise facial e aprendizado profundo\n",
        "!pip install -q deepface face_recognition tf-keras"
      ],
      "metadata": {
        "id": "Vdh822cdiCdG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "from deepface import DeepFace\n",
        "from tqdm import tqdm\n",
        "import face_recognition\n",
        "\n",
        "# Inicializar MediaPipe\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_pose = mp.solutions.pose"
      ],
      "metadata": {
        "id": "BtT4PTpyiEEU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_video(input_path, output_path):\n",
        "    \"\"\"\n",
        "    Inicializa a captura do v√≠deo de entrada e configura o v√≠deo de sa√≠da.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): Caminho para o arquivo de v√≠deo de entrada.\n",
        "        output_path (str): Caminho para o arquivo de v√≠deo de sa√≠da.\n",
        "\n",
        "    Returns:\n",
        "        cap (cv2.VideoCapture): Objeto de captura para o v√≠deo de entrada.\n",
        "        out (cv2.VideoWriter): Objeto de grava√ß√£o para o v√≠deo de sa√≠da.\n",
        "        total_frames (int): N√∫mero total de frames no v√≠deo de entrada.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: Caso o v√≠deo de entrada n√£o possa ser aberto.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        raise FileNotFoundError(f\"N√£o foi poss√≠vel abrir o v√≠deo de entrada: {input_path}\")\n",
        "\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(\n",
        "        output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height)\n",
        "    )\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    return cap, out, total_frames"
      ],
      "metadata": {
        "id": "4D9EgPf_iGrF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Marca√ß√µes de detec√ß√µes"
      ],
      "metadata": {
        "id": "jh_J9PIds760"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_label(frame, name, emotion, gesture, left, top, right, bottom):\n",
        "    \"\"\"\n",
        "    Desenha um r√≥tulo no frame para exibir informa√ß√µes como nome, emo√ß√£o e gesto.\n",
        "\n",
        "    Args:\n",
        "        frame (numpy.ndarray): Quadro atual do v√≠deo onde os r√≥tulos ser√£o desenhados.\n",
        "        name (str): Nome ou identificador da pessoa detectada.\n",
        "        emotion (str): Emo√ß√£o identificada pela an√°lise facial.\n",
        "        gesture (str): Gesto ou atividade detectada.\n",
        "        left (int): Coordenada x superior esquerda do ret√¢ngulo delimitador.\n",
        "        top (int): Coordenada y superior esquerda do ret√¢ngulo delimitador.\n",
        "        right (int): Coordenada x inferior direita do ret√¢ngulo delimitador.\n",
        "        bottom (int): Coordenada y inferior direita do ret√¢ngulo delimitador.\n",
        "\n",
        "    Returns:\n",
        "        None: A fun√ß√£o modifica o frame diretamente.\n",
        "    \"\"\"\n",
        "    # Defini√ß√£o das cores para os textos\n",
        "    name_color = (0, 0, 255)  # Vermelho\n",
        "    emotion_color = (0, 255, 0)  # Verde\n",
        "    gesture_color = (255, 255, 0)  # Amarelo claro\n",
        "\n",
        "    # Textos a serem exibidos\n",
        "    name_text = f\"Name: {name}\"\n",
        "    emotion_text = f\"Emotion: {emotion}\"\n",
        "    gesture_text = f\"Gesture: {gesture}\"\n",
        "\n",
        "    # Configura√ß√µes de fonte e estilo\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale_name = 0.7\n",
        "    font_scale_other = 0.5\n",
        "    thickness = 2\n",
        "\n",
        "    # Calcular dimens√µes dos textos\n",
        "    (name_w, name_h), _ = cv2.getTextSize(name_text, font, font_scale_name, thickness)\n",
        "    (emotion_w, emotion_h), _ = cv2.getTextSize(emotion_text, font, font_scale_other, thickness)\n",
        "    (gesture_w, gesture_h), _ = cv2.getTextSize(gesture_text, font, font_scale_other, thickness)\n",
        "\n",
        "    # Dimens√µes do fundo para os textos\n",
        "    rect_width = max(name_w, emotion_w, gesture_w) + 10\n",
        "    rect_height = name_h + emotion_h + gesture_h + 20\n",
        "\n",
        "    # Determinar posi√ß√£o do fundo\n",
        "    rect_left = left\n",
        "    rect_top = top - rect_height if top - rect_height > 0 else top + rect_height\n",
        "    rect_right = rect_left + rect_width\n",
        "    rect_bottom = rect_top + rect_height\n",
        "\n",
        "    # Desenhar fundo preto para os textos\n",
        "    cv2.rectangle(frame, (rect_left, rect_top), (rect_right, rect_bottom), (0, 0, 0), -1)\n",
        "\n",
        "    # Desenhar os textos\n",
        "    cv2.putText(frame, name_text, (rect_left + 5, rect_top + name_h), font, font_scale_name, name_color, thickness)\n",
        "    cv2.putText(frame, emotion_text, (rect_left + 5, rect_top + name_h + emotion_h + 5), font, font_scale_other, emotion_color, thickness)\n",
        "    cv2.putText(frame, gesture_text, (rect_left + 5, rect_top + name_h + emotion_h + gesture_h + 10), font, font_scale_other, gesture_color, thickness)"
      ],
      "metadata": {
        "id": "6AT-McugiJGA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Detec√ß√£o de faces e emo√ß√µes"
      ],
      "metadata": {
        "id": "8mbJuFXptJpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_faces_and_emotions(frame, rgb_frame, face_detection, report, known_faces):\n",
        "    \"\"\"\n",
        "    Detecta rostos e emo√ß√µes em um frame.\n",
        "\n",
        "    Args:\n",
        "        frame (numpy.ndarray): Frame original do v√≠deo.\n",
        "        rgb_frame (numpy.ndarray): Frame convertido para RGB.\n",
        "        face_detection: Objeto do MediaPipe para detec√ß√£o de rostos.\n",
        "        report (dict): Dicion√°rio para armazenar informa√ß√µes agregadas de emo√ß√µes detectadas.\n",
        "        known_faces (list): Lista de codifica√ß√µes faciais conhecidas para reconhecimento.\n",
        "\n",
        "    Returns:\n",
        "        detected_faces (list): Lista de dicion√°rios contendo informa√ß√µes dos rostos detectados.\n",
        "            Cada dicion√°rio cont√©m:\n",
        "                - \"coords\" (tuple): Coordenadas do ret√¢ngulo delimitador (xmin, ymin, xmax, ymax).\n",
        "                - \"name\" (str): Nome ou identificador da pessoa.\n",
        "                - \"emotion\" (str): Emo√ß√£o dominante detectada.\n",
        "                - \"gesture\" (str): Gesto detectado (placeholder como \"None\").\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: Caso o v√≠deo de entrada n√£o possa ser aberto.\n",
        "    \"\"\"\n",
        "    face_results = face_detection.process(rgb_frame)\n",
        "    detected_faces = []\n",
        "    new_face_id = len(known_faces) + 1\n",
        "\n",
        "    if face_results.detections:\n",
        "        for detection in face_results.detections:\n",
        "            bbox = detection.location_data.relative_bounding_box\n",
        "            h, w, _ = frame.shape\n",
        "            xmin = max(0, int(bbox.xmin * w))\n",
        "            ymin = max(0, int(bbox.ymin * h))\n",
        "            xmax = min(w, int((bbox.xmin + bbox.width) * w))\n",
        "            ymax = min(h, int((bbox.ymin + bbox.height) * h))\n",
        "\n",
        "            if xmax <= xmin or ymax <= ymin:\n",
        "                continue\n",
        "\n",
        "            face_roi = rgb_frame[ymin:ymax, xmin:xmax]\n",
        "            face_encoding = face_recognition.face_encodings(rgb_frame, [(ymin, xmax, ymax, xmin)])\n",
        "            name = \"Unknown\"\n",
        "\n",
        "            if face_encoding:\n",
        "                face_encoding = face_encoding[0]\n",
        "                matches = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.6)\n",
        "\n",
        "                if True in matches:\n",
        "                    name = f\"Pessoa {matches.index(True) + 1}\"\n",
        "                else:\n",
        "                    known_faces.append(face_encoding)\n",
        "                    name = f\"Pessoa {new_face_id}\"\n",
        "                    new_face_id += 1\n",
        "\n",
        "            try:\n",
        "                emotion_analysis = DeepFace.analyze(face_roi, actions=[\"emotion\"], enforce_detection=False)\n",
        "                dominant_emotion = emotion_analysis[0].get(\"dominant_emotion\", \"Unknown\")\n",
        "                report[\"emotion_summary\"][name] = report[\"emotion_summary\"].get(name, {})\n",
        "                report[\"emotion_summary\"][name][dominant_emotion] = (\n",
        "                    report[\"emotion_summary\"][name].get(dominant_emotion, 0) + 1\n",
        "                )\n",
        "                detected_faces.append({\"coords\": (xmin, ymin, xmax, ymax), \"name\": name, \"emotion\": dominant_emotion, \"gesture\": \"None\"})\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao analisar emo√ß√£o: {e}\")\n",
        "\n",
        "    return detected_faces"
      ],
      "metadata": {
        "id": "aU9GpussiMCm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Detec√ß√£o de gestos"
      ],
      "metadata": {
        "id": "33Niuuq8tS8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_poses(frame, pose_detection, detected_faces, report):\n",
        "    \"\"\"\n",
        "    Detecta gestos e associa aos rostos detectados no frame.\n",
        "\n",
        "    Args:\n",
        "        frame (numpy.ndarray): Frame atual do v√≠deo para an√°lise.\n",
        "        pose_detection: Objeto do MediaPipe para detec√ß√£o de poses corporais.\n",
        "        detected_faces (list): Lista de rostos detectados contendo coordenadas e outras informa√ß√µes.\n",
        "        report (dict): Dicion√°rio para armazenar informa√ß√µes agregadas de gestos detectados.\n",
        "\n",
        "    Returns:\n",
        "        None: A fun√ß√£o modifica diretamente os argumentos `detected_faces`, `report` e `frame`.\n",
        "    \"\"\"\n",
        "    # Processar o frame para detectar poses\n",
        "    pose_results = pose_detection.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    if pose_results.pose_landmarks:\n",
        "        landmarks = pose_results.pose_landmarks.landmark\n",
        "\n",
        "        for face in detected_faces:\n",
        "            xmin, ymin, xmax, ymax = face[\"coords\"]\n",
        "\n",
        "            # Inicializar gesto como \"None\"\n",
        "            face[\"gesture\"] = \"None\"\n",
        "\n",
        "            # Detectar gesto \"Wave\" (aceno)\n",
        "            if (landmarks[mp_pose.PoseLandmark.LEFT_WRIST].y < landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y or\n",
        "                landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].y < landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].y):\n",
        "                face[\"gesture\"] = \"Wave\"\n",
        "                report[\"gestures\"][\"wave\"] = report[\"gestures\"].get(\"wave\", 0) + 1\n",
        "\n",
        "            # Detectar gesto \"Hands Up\" (m√£os acima da cabe√ßa)\n",
        "            if (landmarks[mp_pose.PoseLandmark.LEFT_WRIST].y < landmarks[mp_pose.PoseLandmark.NOSE].y and\n",
        "                landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].y < landmarks[mp_pose.PoseLandmark.NOSE].y):\n",
        "                face[\"gesture\"] = \"Hands Up\"\n",
        "                report[\"gestures\"][\"hands_up\"] = report[\"gestures\"].get(\"hands_up\", 0) + 1\n",
        "\n",
        "            # Detectar gesto \"Arms Crossed\" (bra√ßos cruzados)\n",
        "            if (landmarks[mp_pose.PoseLandmark.LEFT_WRIST].x > landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW].x and\n",
        "                landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].x < landmarks[mp_pose.PoseLandmark.LEFT_ELBOW].x):\n",
        "                face[\"gesture\"] = \"Arms Crossed\"\n",
        "                report[\"gestures\"][\"arms_crossed\"] = report[\"gestures\"].get(\"arms_crossed\", 0) + 1\n",
        "\n",
        "            # Detectar gesto \"Pointing\" (apontando com a m√£o direita)\n",
        "            if (landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].x > landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW].x and\n",
        "                landmarks[mp_pose.PoseLandmark.RIGHT_INDEX].x > landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].x):\n",
        "                face[\"gesture\"] = \"Pointing\"\n",
        "                report[\"gestures\"][\"pointing\"] = report[\"gestures\"].get(\"pointing\", 0) + 1\n",
        "\n",
        "        # Desenhar landmarks no frame para visualiza√ß√£o\n",
        "        mp_drawing.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
        "\n",
        "    else:\n",
        "        # Se nenhum landmark for detectado, inicializar gestos no relat√≥rio\n",
        "        for gesture in [\"wave\", \"hands_up\", \"arms_crossed\", \"pointing\"]:\n",
        "            report[\"gestures\"][gesture] = report[\"gestures\"].get(gesture, 0)"
      ],
      "metadata": {
        "id": "gXYqkt5yiPE1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_anomalies(frame, detected_faces, pose_results, report):\n",
        "    \"\"\"\n",
        "    Detecta anomalias no comportamento com base nos landmarks corporais e gestos.\n",
        "\n",
        "    Args:\n",
        "        frame (ndarray): Frame atual do v√≠deo.\n",
        "        detected_faces (list): Lista de rostos detectados no frame.\n",
        "        pose_results: Resultados da detec√ß√£o de pose (MediaPipe).\n",
        "        report (dict): Relat√≥rio de an√°lise do v√≠deo.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    if pose_results.pose_landmarks:\n",
        "        landmarks = pose_results.pose_landmarks.landmark\n",
        "\n",
        "        for face in detected_faces:\n",
        "            xmin, ymin, xmax, ymax = face[\"coords\"]\n",
        "\n",
        "            # Detectar anomalias como movimentos bruscos ou fora de padr√£o\n",
        "            if (\n",
        "                landmarks[mp_pose.PoseLandmark.LEFT_WRIST].y < 0 or  # Posi√ß√£o fora do alcance\n",
        "                landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].y < 0 or\n",
        "                abs(landmarks[mp_pose.PoseLandmark.LEFT_WRIST].y - landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].y) > 0.5\n",
        "            ):\n",
        "                report[\"anomalies_detected\"] += 1\n",
        "\n",
        "                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
        "                cv2.putText(frame, \"Anomaly Detected\", (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)"
      ],
      "metadata": {
        "id": "h41b0FO0_7Ms"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Gera√ß√£o de Relat√≥rio"
      ],
      "metadata": {
        "id": "3Gca5-bqtict"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucZ-1PtYtwV_",
        "outputId": "23261fc2-45ac-41d5-e9c3-76afa1e406d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=b81a4a595a4dc4dbc1e79741a2f13d87e3ffcaddd1e025c2d62843adbd91d2a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_pie_chart(data, labels, title, output_path):\n",
        "    \"\"\"\n",
        "    Gera um gr√°fico de pizza com os dados fornecidos.\n",
        "\n",
        "    Args:\n",
        "        data (list): Valores dos segmentos.\n",
        "        labels (list): R√≥tulos dos segmentos.\n",
        "        title (str): T√≠tulo do gr√°fico.\n",
        "        output_path (str): Caminho para salvar o gr√°fico.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.pie(data, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "    plt.title(title)\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "\n",
        "def save_report(report, report_path):\n",
        "    \"\"\"\n",
        "    Gera um relat√≥rio em PDF com informa√ß√µes de an√°lise de v√≠deo.\n",
        "\n",
        "    Args:\n",
        "        report (dict): Dicion√°rio contendo informa√ß√µes do relat√≥rio.\n",
        "        report_path (str): Caminho para salvar o arquivo PDF.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "    # T√≠tulo do relat√≥rio\n",
        "    pdf.set_font(\"Arial\", style=\"B\", size=16)\n",
        "    pdf.cell(200, 10, txt=\"Relat√≥rio de An√°lise de V√≠deo\", ln=True, align='C')\n",
        "    pdf.ln(10)  # Espa√ßamento\n",
        "\n",
        "    # Informa√ß√µes gerais\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.cell(200, 10, txt=f\"Total de Frames Analisados: {report['total_frames_analyzed']}\", ln=True)\n",
        "    pdf.cell(200, 10, txt=f\"Total de Rostos Detectados: {len(report['emotion_summary'])}\", ln=True)\n",
        "    pdf.cell(200, 10, txt=f\"Total de Anomalias Detectadas: {report['anomalies_detected']}\", ln=True)\n",
        "    pdf.ln(15)  # Espa√ßamento antes do gr√°fico\n",
        "\n",
        "    # Gr√°fico de emo√ß√µes\n",
        "    pdf.cell(200, 10, txt=\"Propor√ß√£o de Emo√ß√µes Detectadas:\", ln=True)\n",
        "    pdf.ln(5)\n",
        "    pdf.image(\"emotions_pie.png\", x=60, y=pdf.get_y(), w=90)\n",
        "    pdf.ln(70)\n",
        "\n",
        "    # Gr√°fico de gestos\n",
        "    pdf.cell(200, 10, txt=\"Propor√ß√£o de Gestos Reconhecidos:\", ln=True)\n",
        "    pdf.ln(5)\n",
        "    pdf.image(\"gestures_pie.png\", x=60, y=pdf.get_y(), w=90)\n",
        "    pdf.ln(70)\n",
        "\n",
        "    # Salvar o arquivo PDF\n",
        "    pdf.output(report_path)"
      ],
      "metadata": {
        "id": "--jugqoriTZu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Processamento do v√≠deo"
      ],
      "metadata": {
        "id": "YbsyvMOGtloE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(input_video, output_video, report_path):\n",
        "    \"\"\"\n",
        "    Processa o v√≠deo para detectar rostos, emo√ß√µes, gestos e gerar um relat√≥rio em PDF.\n",
        "\n",
        "    Args:\n",
        "        input_video (str): Caminho para o v√≠deo de entrada.\n",
        "        output_video (str): Caminho para salvar o v√≠deo de sa√≠da processado.\n",
        "        report_path (str): Caminho para salvar o relat√≥rio em PDF.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Inicializar v√≠deo e capturas\n",
        "    cap, out, total_frames = initialize_video(input_video, output_video)\n",
        "\n",
        "    # Inicializar MediaPipe e outras configura√ß√µes\n",
        "    face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
        "    pose_detection = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "\n",
        "    known_faces = []  # Lista para armazenar codifica√ß√µes faciais conhecidas\n",
        "    report = {\n",
        "        \"total_faces_detected\": 0,\n",
        "        \"emotion_summary\": {},  # Armazena contagens de emo√ß√µes por pessoa\n",
        "        \"gestures\": {\"wave\": 0, \"hands_up\": 0, \"arms_crossed\": 0, \"pointing\": 0},\n",
        "        \"total_frames_analyzed\": 0,\n",
        "        \"anomalies_detected\": 0\n",
        "\n",
        "    }\n",
        "\n",
        "    # Configurar barra de progresso\n",
        "    progress_bar = tqdm(total=total_frames, desc=\"Processando v√≠deo\", unit=\"frame\")\n",
        "\n",
        "    try:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            report[\"total_frames_analyzed\"] += 1\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Detectar rostos e emo√ß√µes\n",
        "            detected_faces = detect_faces_and_emotions(frame, rgb_frame, face_detection, report, known_faces)\n",
        "\n",
        "            # Detectar gestos e associar aos rostos\n",
        "            detect_poses(frame, pose_detection, detected_faces, report)\n",
        "\n",
        "            # Detectar anomalias no comportamento\n",
        "            detect_anomalies(frame, detected_faces, pose_results=pose_detection.process(rgb_frame), report=report)\n",
        "\n",
        "            # Desenhar informa√ß√µes no frame\n",
        "            for face in detected_faces:\n",
        "                draw_label(frame, face[\"name\"], face[\"emotion\"], face[\"gesture\"], *face[\"coords\"])\n",
        "\n",
        "            # Escrever frame processado no v√≠deo de sa√≠da\n",
        "            out.write(frame)\n",
        "            progress_bar.update(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro durante o processamento do v√≠deo: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Garantir que os recursos sejam liberados corretamente\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        progress_bar.close()\n",
        "\n",
        "    # Salvar relat√≥rio em PDF\n",
        "    save_report(report, report_path)\n"
      ],
      "metadata": {
        "id": "coHtkQ5giWxB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminhos de entrada e sa√≠da\n",
        "input_video = \"/content/video.mp4\"\n",
        "output_video = \"/content/output_video.mp4\"\n",
        "report_path = \"/content/report.pdf\"\n",
        "\n",
        "# Processar o v√≠deo\n",
        "process_video(input_video, output_video, report_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUqSGRhLiZDZ",
        "outputId": "80af7b41-848c-4dea-8a09-d41c9b2f81dd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processando v√≠deo: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3326/3326 [06:18<00:00,  8.78frame/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. Conclus√£o\n",
        "\n",
        "O Tech Challenge da fase 4 exigiu o desenvolvimento de uma aplica√ß√£o robusta para an√°lise de v√≠deo, utilizando t√©cnicas avan√ßadas de vis√£o computacional e aprendizado de m√°quina. O projeto alcan√ßou seus objetivos, implementando funcionalidades de reconhecimento facial, an√°lise de express√µes emocionais, detec√ß√£o de gestos e gera√ß√£o de relat√≥rios automatizados.\n",
        "\n",
        "**Principais Resultados**\n",
        "\n",
        "1.\t**An√°lise de Frames e Detec√ß√£o de Rostos:**\n",
        "\n",
        "- Total de frames analisados: **3.326**.\n",
        "\n",
        "- Total de rostos detectados: **22 indiv√≠duos**, com emo√ß√µes e gestos identificados.\n",
        "\n",
        "- Total de anomalias detectadas:  **259**.\n",
        "\n",
        "2.\t**An√°lise de Express√µes Emocionais:**\n",
        "\n",
        "- As emo√ß√µes foram distribu√≠das entre felicidade, tristeza, medo e neutralidade, mostrando boa cobertura de estados emocionais.\n",
        "\n",
        "3.\t**Detec√ß√£o de Gestos:**\n",
        "\n",
        "- Foram detectados gestos de acenos, bra√ßos cruzados, m√£os acima da cabe√ßa e movimento de apontar com a m√£o.\n",
        "\n",
        "4.\t**Relat√≥rios Automatizados:**\n",
        "\n",
        "- Relat√≥rio gerado em PDF consolidou informa√ß√µes claras e organizadas, permitindo uma an√°lise detalhada e objetiva.\n",
        "\n",
        "**Limita√ß√µes Identificadas**\n",
        "\n",
        "1.\t**Qualidade de Imagem:**\n",
        "\n",
        "- Frames com baixa ilumina√ß√£o ou movimentos r√°pidos afetaram a precis√£o de algumas an√°lises.\n",
        "\n",
        "2.\t**Generaliza√ß√£o dos Modelos:**\n",
        "\n",
        "- O sistema teve dificuldades em cen√°rios mais complexos, como intera√ß√µes n√£o previstas.\n",
        "\n",
        "**Trabalhos Futuros**\n",
        "\n",
        "1.\t**Aprimorar Pr√©-processamento:**\n",
        "\n",
        "- Aplicar filtros avan√ßados para melhorar a qualidade dos frames.\n",
        "\n",
        "2.\t**Expandir os Cen√°rios:**\n",
        "\n",
        "- Testar o sistema com v√≠deos que incluam mais diversidade de condi√ß√µes e atividades.\n",
        "\n",
        "3.\t**Otimizar o Algoritmo:**\n",
        "\n",
        "- Implementar melhorias de desempenho, como processamento paralelo e uso de modelos mais recentes.\n",
        "\n",
        "4.\t**Adicionar Novos Gestos:**\n",
        "\n",
        "- Ampliar a detec√ß√£o para cobrir intera√ß√µes mais complexas e gestos at√≠picos.\n",
        "\n",
        "####Conclus√£o Final\n",
        "\n",
        "O projeto demonstrou como integrar t√©cnicas de intelig√™ncia artificial para criar uma solu√ß√£o eficiente e pr√°tica de an√°lise de v√≠deo. Com alta precis√£o em reconhecimento facial, an√°lise emocional e detec√ß√£o de atividades, o sistema mostrou seu potencial para aplica√ß√µes em seguran√ßa, marketing, sa√∫de e entretenimento. Trabalhos futuros podem melhorar ainda mais sua efic√°cia e generaliza√ß√£o, abrindo espa√ßo para aplica√ß√µes em escala e cen√°rios mais desafiadores."
      ],
      "metadata": {
        "id": "JXsHa1bv1izi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}